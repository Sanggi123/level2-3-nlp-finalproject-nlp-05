{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f4vrUGkBp7DZ"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2Config, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rJWI3XmoqQW9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AKI_hmnnqjPo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"output/checkpoint-13500/\").to(device)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = '깊은 밤의 불안한 별빛'\n",
        "input_ids = tokenizer.encode(sentence, add_special_tokens=True, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        temperature=0.2, # 생성 다양성 조절\n",
        "        max_new_tokens=64, # 생성되는 문장의 최대 길이\n",
        "        top_k=25, # 높은 확률을 가진 top-k 토큰만 고려\n",
        "        top_p=0.95, # 누적 확률이 p를 초과하는 토큰은 제외\n",
        "        repetition_penalty=1.2, # 반복을 줄이는 패널티\n",
        "        do_sample=True, # 샘플링 기반 생성 활성화\n",
        "        num_return_sequences=2, # 생성할 시퀀스의 수\n",
        "    )\n",
        "\n",
        "text_list = []\n",
        "for output_text in output:\n",
        "    text_list.append(tokenizer.decode(output_text.tolist(), skip_special_tokens=True))\n",
        "\n",
        "# generated_sequence = output[0].tolist()\n",
        "# text = tokenizer.decode(generated_sequence, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------1번째 시----------\n",
            "깊은 밤의 불안한 별빛에\n",
            "그 떨림이 멈추어 있는 \n",
            "별들의 마음에도\n",
            "이렇게 찬란하도록 빛나는 시간이 있음을 알게 됩니다\n",
            "그래서 아름다운 날에요\n",
            "고운 새들이 가져온\n",
            "하얀 눈송이가 들려주는\n",
            "꿈의 이야기들\n",
            "그리고 달팽이의 하얀 눈이 내리는 곳마다\n",
            "아름다움의 깊이\n",
            "----------2번째 시----------\n",
            "깊은 밤의 불안한 별빛에\n",
            "어둠이 스며들면\n",
            "그렇게 꿈은 사라지고 \n",
            "아름다움의 시간 속에 있는\n",
            "고운 새들의 아침과 저녁놀의 시작을 알려줍니다\n",
            "그래서 아름다운 날에요\n",
            "별들이 보내온\n",
            "하얀 눈송이의 편지 안에\n",
            "꿈으로 가는 길에는\n",
            "마음이 닿는\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(text_list)):\n",
        "    print(f\"----------{i+1}번째 시----------\")\n",
        "    print(text_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = '돈'\n",
        "toked = tokenizer(\"</s>\" + sentence)\n",
        "temp = []\n",
        "cnt = 0\n",
        "while True:\n",
        "  input_ids = torch.tensor(toked[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "  pred = model(input_ids)[0]\n",
        "\n",
        "  gen = tokenizer.decode(torch.argmax(pred, axis=-1).squeeze().tolist())\n",
        "  print(gen)\n",
        "  print(\"선택한 단어 : \", gen[-1])\n",
        "  gen = gen[-1]\n",
        "  cnt += 1\n",
        "\n",
        "  if cnt == 50:\n",
        "    break\n",
        "\n",
        "  if '' == gen:\n",
        "    break\n",
        "  sentence += gen.replace('▁', ' ')\n",
        "  toked = tokenizer(sentence)\n",
        "\n",
        "print(sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN+1o7ZT89AaH0RyjJRww3h",
      "include_colab_link": true,
      "name": "3. Generater.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
